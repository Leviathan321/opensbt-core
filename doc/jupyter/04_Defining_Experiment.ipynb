{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Configuration and Execution\n",
    "\n",
    "In this tutorial we show how to configure and execute an experiment in OpenSBT to test an Agent. We show the configuration of the experiment for testing a simplistic agent in a simplified simulator (no GPU required).\n",
    "\n",
    "_Note: This example is for introducing OpenSBT. An application with a high fidelity simulation and *real* SUT is provided [here](doc)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Example\n",
    "\n",
    "We refer to this example as Dummy Example. The example contains an AEB agent and a pedestrian. Both actors move linear, while the pedestrian has a fixed trajectory and fixed velocity, the ego agent has a predefined path but can modify his velocity. That means, ego brakes when he detects other actors within a predefined distance. We want to test the AEB agent in violating the safety distance of 1m. For this we want to vary the initial velocities of both actors as well the orientation of the pedestrian crossing egos' lane.\n",
    "\n",
    "In particlar, the pedestrian has a speed in the range of [0.2, 3] m/s, the ego [1, 10] m/s, while the orientation range is [0,180] degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining An Experiment\n",
    "\n",
    "To define an experiment we need to instantiate `ADASProblem`. `ADASProblem` holds:\n",
    "\n",
    "- the **scenario**, which can be e.g., an OpenSCENARIO file, or file of another format supported by simulator,\n",
    "- the **search variables**, whose values are altered to produce (different) scenarios througout the search,\n",
    "- the **search space**, represented by the lower and upper bounds as arrays,\n",
    "- the **fitness function**, \n",
    "- the **criticality function**, which represents the safety requirements/testing oracle,\n",
    "- the **simulate function** to trigger the simulator via this function,\n",
    "- the **simulator-related variables**, i.e., simulation time, sampling time, toggle for visualization.\n",
    "\n",
    "_Note, that for now the SUT is triggered via the Simulater Interface, as we assume that the SUT is embedded into the simulator. We are working on an update where the SUT can be defined uncoupled from the simulator._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'opensbt-core'\n",
      "/home/sorokin/Projects/testing/search-based-test-case-generation/doc/jupyter/opensbt-core\n"
     ]
    }
   ],
   "source": [
    "cd opensbt-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from evaluation.fitness import *\n",
    "from problem.adas_problem import ADASProblem\n",
    "from problem.pymoo_test_problem import PymooTestProblem\n",
    "from experiment.experiment_store import *\n",
    "from algorithm.algorithm import *\n",
    "from evaluation.critical import *\n",
    "from simulation.dummy_simulation import DummySimulator\n",
    "\n",
    "\n",
    "problem = ADASProblem(\n",
    "                      problem_name=\"DummySimulatorProblem\",\n",
    "                      scenario_path=\"scenarios/dummy_scenario.xosc\",\n",
    "                      xl=[0, 1, 0, 1],\n",
    "                      xu=[360, 3,360, 3],\n",
    "                      simulation_variables=[\n",
    "                          \"orientation_ego\",\n",
    "                          \"velocity_ego\",\n",
    "                          \"orientation_ped\",\n",
    "                          \"velocity_ped\"],\n",
    "                      fitness_function=FitnessMinDistanceVelocity(),\n",
    "                      critical_function=CriticalAdasDistanceVelocity(),\n",
    "                      simulate_function=DummySimulator.simulate,\n",
    "                      simulation_time=10,\n",
    "                      sampling_time=0.25\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two options to execute the testing experiment: using the console or using the code directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Execution (Code)\n",
    "\n",
    "To start search without console, we instantiate the search algorithms `Optimizer` and call its `run` method to start the search. When the search has finished, we write the results by calling an output method (line 9). The results will be written in the default folder name `results`, if no folder name is passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |       20 |      3 |             - |             -\n",
      "     2 |       40 |      4 |  0.0021865537 |             f\n",
      "     3 |       60 |      3 |  0.0299531884 |         nadir\n",
      "     4 |       80 |      6 |  0.0094474054 |         ideal\n",
      "     5 |      100 |      5 |  0.0076966263 |         ideal\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Result' object has no attribute 'write_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m res \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Write results\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_results\u001b[49m(params \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mparameters)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Result' object has no attribute 'write_results'"
     ]
    }
   ],
   "source": [
    "from algorithm.nsga2_optimizer import NsgaIIOptimizer\n",
    "\n",
    "# Instantiate search algorithm\n",
    "optimizer = NsgaIIOptimizer(\n",
    "                            problem=problem,\n",
    "                            config=DefaultSearchConfiguration()\n",
    "                            )\n",
    "# Run search\n",
    "res = optimizer.run()\n",
    "\n",
    "# Write results\n",
    "res.write_results(params = optimizer.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Execution (Console)\n",
    "\n",
    "To start the search through the console we need to create an `Experiment` instance. The experiment instance receives following inputs:\n",
    "\n",
    "- the experiment name (type `str`)\n",
    "- the created problem (type `ADASProblem`) \n",
    "- the search algorithm (type `AlgorithmType`)\n",
    "- the search configuration (type `SearchConfiguration`)\n",
    "\n",
    "Note, that the algorithm that is passed is an enumeration value which need to be registed beforehand via the `algorithm.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(name=\"6\",\n",
    "                        problem=problem,\n",
    "                        algorithm=AlgorithmType.NSGAII,\n",
    "                        search_configuration=DefaultSearchConfiguration())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we register the experiment so that we can use it later via console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_store.register(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start search via console we run the experiment with the name \"6\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-15 23:41:54,418 INFO     Logging setup. Writing to file:  ./log.txt\r\n",
      "2023-10-15 23:41:54,419 INFO     Selected experiment: 6\r\n",
      "2023-10-15 23:41:54,419 INFO     Experiment with the given name does not exist.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"run.py\", line 101, in <module>\r\n",
      "    config = experiment.search_configuration\r\n",
      "AttributeError: 'int' object has no attribute 'search_configuration'\r\n"
     ]
    }
   ],
   "source": [
    "!python run.py -e 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the experiment parameters, such e.g., lower and upper bounds of the search parameters, the search time, iteration size or population size by using flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-15 23:42:03,969 INFO     Logging setup. Writing to file:  ./log.txt\r\n",
      "2023-10-15 23:42:03,970 INFO     Selected experiment: 6\r\n",
      "2023-10-15 23:42:03,970 INFO     Experiment with the given name does not exist.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"run.py\", line 101, in <module>\r\n",
      "    config = experiment.search_configuration\r\n",
      "AttributeError: 'int' object has no attribute 'search_configuration'\r\n"
     ]
    }
   ],
   "source": [
    "!python run.py -e 6 -min 3 3 -max 4 4 -m  \"velocity_ego\" \"velocity_ego\" -i 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete list of flags is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    " -h, --help            show this help message and exit\n",
    "  -e EXP_NUMBER         Name of existing experiment to be used. (show all experiments via -info)].\n",
    "  -i N_GENERATIONS      Number generations to perform.\n",
    "  -n SIZE_POPULATION    The size of the initial population of scenario candidates.\n",
    "  -a ALGORITHM          The algorithm to use for search. (Currently only 1: NSGAII supported.)\n",
    "  -t MAXIMAL_EXECUTION_TIME\n",
    "                        The time to use for search.\n",
    "  -f SCENARIO_PATH      The path to the scenario description file.\n",
    "  -min VAR_MIN [VAR_MIN ...]\n",
    "                        The lower bound of each search parameter.\n",
    "  -max VAR_MAX [VAR_MAX ...]\n",
    "                        The upper bound of each search parameter.\n",
    "  -m DESIGN_NAMES [DESIGN_NAMES ...]\n",
    "                        The names of the variables to modify.\n",
    "  -o RESULTS_FOLDER     The name of the folder where the results of the search are stored (default: /results/single/)\n",
    "  -v                    Whether to use the simuator's visualization. This feature is useful for debugging and demonstrations, however it reduces the search performance.\n",
    "  -info                 Names of all defined experiments.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "936ca7e20bca62b66c7c316e70494dc6b833065bb3866688df5c5758a2e02a05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
